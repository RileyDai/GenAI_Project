# MSAI 495 Assignment: Image Generation

This project implements a Variational Autoencoder (VAE) to learn and generate images.

## Project Structure

```text
â”œâ”€â”€ best_models/ # Stores best-performing model checkpoints (e.g., lowest FID)
â”œâ”€â”€ config/ # YAML config files
â”œâ”€â”€ data_apples/ # Apple Dataset root folder 
â”œâ”€â”€ data_impressionism/ # Apple Dataset root folder 
â”œâ”€â”€ logs/ # Training logs and checkpoints 
â”œâ”€â”€ run_vae.py # VAE model architecture
â”œâ”€â”€ tune_vae.py # Ray Tune
â”œâ”€â”€ ui.py # interactive Streamlit UI
â”œâ”€â”€ README.md 
â””â”€â”€ environment.yml # Conda environment specification 
```

## Datasets

This project utilizes two datasets:

### 1. WikiArt Impressionism Subset

- A dataset focused on Impressionism style paintings from [WikiArt](https://www.kaggle.com/datasets/steubk/wikiart).
- Categorized into 4 visual genres:
  - `landscape` : 3079
  - `genre_painting` : 2669
  - `portrait` : 1899
  - `cityscape` : 1545

### 2. Fruit360 Apple Subset

- A simpler dataset composed of Apple class images from [Fruit360](https://www.kaggle.com/datasets/moltean/fruits).
- Used as a **control experiment** to validate basic image generation capability of the model.
- This subset includes 3 apple categories:
  - `Golden` : 1938
  - `Golden-Red` : 1647
  - `Red` : 1885
- Download from Google Drive: [Dataset Link](https://drive.google.com/file/d/1PReXv2oG5IiErLhXRoru0oJIF25RCYS8/view?usp=drive_link)

> **Why two datasets?**  
> During implementation, I found that paintings require much finer detail and structure to generate realistic results. In contrast, , Fruit360 is simpler and more structured. It allows the modelâ€™s performance to be shown more directly.

## Getting Started

### 1. Installation

```bash
conda env create -f environment.yml
conda activate genai
```

### 2. Train

```bash
python run_vae.py --config ./config/train_apples
```

### 3. Test

```bash
python run_vae.py --config ./config/test_apples
```

To visually explore the generated samples, you can launch the interactive Streamlit UI:

```bash
streamlit run ui.py
```

## Model Architecture

| Part    | Layer(s)                                                                                                                                          | Notes                                                                                                      |
|---------|---------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|
| Encoder | **Inputâ€¯3â€¯Ã—â€¯Hâ€¯Ã—â€¯W** â†’ Conv2dâ€¯(3â€¯â†’â€¯64,â€¯4Ã—4,â€¯strideâ€¯2,â€¯padâ€¯1) â†’ ReLU â†’ Conv2dâ€¯(64â€¯â†’â€¯128,â€¯4Ã—4,â€¯s2) â†’ BatchNorm â†’ ReLU â†’ Conv2dâ€¯(128â€¯â†’â€¯256,â€¯4Ã—4,â€¯s2) â†’ BatchNorm â†’ ReLU â†’ Conv2dâ€¯(256â€¯â†’â€¯512,â€¯4Ã—4,â€¯s2) â†’ BatchNorm â†’ ReLU â†’ Flatten â†’ **FCâ€¯â†’â€¯Î¼**, **FCâ€¯â†’â€¯logâ€¯ÏƒÂ²** | If `--conditional`, the oneâ€‘hot label (**c**) is broadcast on channels and concatenated with **x** before the first conv. |
| Latent  | **zâ€¯=â€¯Î¼Â +Â Ïƒâ€¯âŠ™â€¯Îµ**, with Îµâ€¯~â€¯ğ’©(0,â€¯I)                                                                                                               | Defaultâ€¯`latent_dim = 128`; sweeps such as {64,â€¯128,â€¯256} supported.                                       |
| Decoder | **z (Â±â€¯c)** â†’ FC â†’ reshape to 512â€¯Ã—â€¯(H/16)â€¯Ã—â€¯(W/16) â†’ ConvTâ€¯(512â€¯â†’â€¯256,â€¯4Ã—4,â€¯s2) â†’ BatchNorm â†’ ReLU â†’ ConvTâ€¯(256â€¯â†’â€¯128,â€¯4Ã—4,â€¯s2) â†’ BatchNorm â†’ ReLU â†’ ConvTâ€¯(128â€¯â†’â€¯64,â€¯4Ã—4,â€¯s2) â†’ BatchNorm â†’ ReLU â†’ ConvTâ€¯(64â€¯â†’â€¯3,â€¯4Ã—4,â€¯s2) â†’ Sigmoid | Outputs RGB in **[0,â€¯1]**; if conditional, **c** is concatenated with **z** at the FC input.               |
| Loss    | **â„’ = MSE(x,â€¯xÌ‚) + Î² Â· KL(ğ’©(Î¼,â€¯ÏƒÂ²)â€–ğ’©(0,â€¯I))**                                                                                                    | `Î²` linearly warms up over `--beta_warmup` epochs and can cycle every `--beta_cycle` epochs.               |

## Extra Project Criteria

### 1.â€¯Metrics Training & Evaluation Tracking

- Utilized **TensorBoard** (`SummaryWriter`) to log key training metrics such as loss and KL divergence.
- Employed **Weights & Biases (wandb)** to track all relevant metrics, including loss, KL, FID, and Î² throughout training.
- Implemented a custom `evaluate_fid()` function that periodically computes **FID** during training and saves the best-performing model based on this metric.

### 2.Â HyperparameterÂ TuningÂ Strategies

- Adopted **Ray Tune with OptunaSearch** for systematic hyperparameter optimization, exploring key parameters such as learning rate, latent dimension, Î², Î² warm-up duration, and batch size.
- Investigated **Î² scheduling strategies**, including **KL annealing** and **cyclic Î²**, controlled via --beta_warmup and --beta_cycle arguments.
- Each trial automatically logs metrics and preserves configuration for reproducibility.

### 3. GalleryÂ GUI

- Enabled reconstruction-based image generation via random sampling from the latent prior z ~ N(0, I), supporting **conditional inputs** y for CVAE models.
- Implemented generation under both **conditional and unconditional settings**, using **one-hot label embeddings** for multi-class controllable generation.

### 4. GalleryÂ GUI

Built an interactive GUI with Streamlit (`ui.py`) to visually explore generated images, allowing users to configure model paths and visualize outputs in real time.

## Results Overview

### Apples Dataset

The model performs well on the **Fruit360 Apples subset**, successfully generating images that resemble **three distinct apple varieties**. The outputs show clear structure, smooth surfaces, and appropriate color variation, indicating that the VAE has effectively learned the data distribution.

**Generated Samples:**

<!-- ![Apples Output](./sample_apples.png) -->
<img src="./sample_apples.png" alt="Apples Output" width="600" height="600"/>

### Impressionism Paintings (WikiArt)

On the **Impressionism subset of WikiArt**, results are **visually aligned with stylistic features**â€”such as **loose brushstrokes** and **vibrant colors**â€”but lack semantic clarity in depicting concrete scenes or objects. This suggests that the current model captures texture but not complex spatial composition.

**Generated Samples:**

<!-- ![Impressionism Output](./sample_impressionism.png) -->
<img src="./sample_impressionism.png" alt="Impressionism Output" width="600" height="600"/>

#### Potential Future Improvements

- **Perceptual reconstruction losses**  
  Replace pixel-wise MSE with VGG-perceptual loss or LPIPS to preserve texture.  
  â†’ Loss format: `L_total = Î»_pix * MSE + Î»_perc * L_VGG + Î² * KL`.

- **Hierarchical / multi-scale latents**  
  Capture global color and fine texture separately with a Ladder VAE or VDVAE.  
  â†’ Upper latents model color wash, lower latents refine strokes.

- **Brush-stroke inductive biases**  
  Default CNNs may miss stroke orientation.  
  â†’ Try Gabor filters, directional kernels, or Vision Transformer decoders.

- **Better priors & posteriors**  
  Move beyond factorized Gaussian assumptions.  
  â†’ Use flow-based posteriors (IAF, Real-NVP) or switch to VQ-VAE-2 for sharper images.

- **Adversarial or diffusion refinements**  
  Boost high-frequency details while keeping Î²-VAE semantics.  
  â†’ Add a patch-based GAN head or feed VAE output into a diffusion upsampler.
